from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.datasets import load_digits

digits = load_digits()


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3) #30% of the data

# logistic regression
log_ = LogisticRegression()
log_.fit(X_train, y_train)
log_.score(X_test, y_test) #0.967%

# linear regression
from sklearn.linear_model import LinearRegression
lin_ = LinearRegression()
lin_.fit(X_train, y_train)
lin_.score(X_test, y_test) #0.527%

# Support Vector Machine
svm_ = SVC()
svm_.fit(X_train, y_train)
svm_.score(X_test, y_test) #0.98%

# random forest
rf_ = RandomForestClassifier()
rf_.fit(X_train, y_train)
rf_.score(X_test, y_test)# 0.977%




# SPLIT THE DATA INTO 3 TRANINING SET SAMPLES
from sklearn.model_selection import KFold

kf = KFold(n_splits=3)
for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):
    print(train_index, test_index)
    
'''
[ training  ] [ test]
[3 4 5 6 7 8] [0 1 2]
[0 1 2 6 7 8] [3 4 5]
[0 1 2 3 4 5] [6 7 8]

#explaination : 012 in first test dataset, but in second 345 become the dataset but 012 become training dataset
'''

# get score
def get_score(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)

print(get_score(LogisticRegression(), X_train, X_test, y_train,y_test))

print(get_score(LinearRegression(), X_train, X_test, y_train, y_test))


from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=3)

scores_logistic = []
scores_svm = []
scores_rf = []

'''for train_index, test_index in kf.split(digits.data):
    X_train, X_test, y_train, y_test = 
    '''
print(digits.data.head(5))
print("HHHH")
print(digits.data[train_index].head(5))
    
    
